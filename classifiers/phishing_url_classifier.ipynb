{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Phishing URL Classifier\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook implements a comprehensive phishing URL classification system that combines multiple approaches:\\n\",\n",
    "    \"1. Traditional feature extraction from URLs and HTML content\\n\",\n",
    "    \"2. BERT-based URL classification\\n\",\n",
    "    \"3. XGBoost model for final classification\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Project Structure\\n\",\n",
    "    \"- `data/`: Contains raw and processed datasets\\n\",\n",
    "    \"- `extractors/`: Contains feature extraction modules\\n\",\n",
    "    \"- `models/`: Saved model files\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Dependencies\\n\",\n",
    "    \"The following packages are required:\\n\",\n",
    "    \"- pandas, numpy: Data manipulation\\n\",\n",
    "    \"- aiohttp: Asynchronous HTTP requests\\n\",\n",
    "    \"- torch, transformers: BERT model\\n\",\n",
    "    \"- scikit-learn: Model evaluation\\n\",\n",
    "    \"- xgboost: Final classifier\\n\",\n",
    "    \"- joblib: Model persistence\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import asyncio\\n\",\n",
    "    \"import aiohttp\\n\",\n",
    "    \"from aioitertools.asyncio import gather\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"from transformers import BertTokenizerFast, BertForSequenceClassification, pipeline\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split, cross_val_score\\n\",\n",
    "    \"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\\n\",\n",
    "    \"import xgboost as xgb\\n\",\n",
    "    \"import joblib\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"from classifiers.extractors.feature_extractor import extract_features_from_url\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set random seeds for reproducibility\\n\",\n",
    "    \"np.random.seed(42)\\n\",\n",
    "    \"torch.manual_seed(42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create necessary directories\\n\",\n",
    "    \"os.makedirs('data', exist_ok=True)\\n\",\n",
    "    \"os.makedirs('models', exist_ok=True)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Data Loading and Preprocessing\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, we'll load and preprocess our datasets. We'll handle both the raw URLs and any existing processed data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def load_datasets():\\n\",\n",
    "    \"    \\\"\\\"\\\"Load and prepare datasets for processing.\\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        urls = pd.read_csv(\\\"data/urls2.csv\\\")\\n\",\n",
    "    \"        print(f\\\"Loaded {len(urls)} URLs from urls2.csv\\\")\\n\",\n",
    "    \"    except FileNotFoundError:\\n\",\n",
    "    \"        print(\\\"urls2.csv not found. Please ensure the file exists in the data directory.\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        handled_urls = pd.read_csv(\\\"data/dataset.csv\\\")\\n\",\n",
    "    \"        print(f\\\"Loaded {len(handled_urls)} processed URLs from dataset.csv\\\")\\n\",\n",
    "    \"    except FileNotFoundError:\\n\",\n",
    "    \"        print(\\\"dataset.csv not found. Creating new dataset.\\\")\\n\",\n",
    "    \"        handled_urls = pd.DataFrame()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return urls, handled_urls\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load datasets\\n\",\n",
    "    \"urls, handled_urls = load_datasets()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Feature Extraction\\n\",\n",
    "    \"\\n\",\n",
    "    \"Extract features from URLs using both traditional methods and HTML content analysis.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"async def get_features(url) -> dict | None:\\n\",\n",
    "    \"    \\\"\\\"\\\"Extract features from a single URL.\\\"\\\"\\\"\\n\",\n",
    "    \"    async with aiohttp.ClientSession() as session:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            res = await session.get(url, timeout=10)\\n\",\n",
    "    \"            valid = True\\n\",\n",
    "    \"            html = await res.text()\\n\",\n",
    "    \"            res.close()\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"Error processing {url}: {str(e)}\\\")\\n\",\n",
    "    \"            valid = False\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not valid:\\n\",\n",
    "    \"        return {\\n\",\n",
    "    \"            \\\"url\\\": url,\\n\",\n",
    "    \"            \\\"status\\\": \\\"invalid\\\",\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    features = extract_features_from_url(url, html)\\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        \\\"url\\\": url\\n\",\n",
    "    \"    } | features | {\\\"status\\\": \\\"legitimate\\\"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"async def validate_urls():\\n\",\n",
    "    \"    \\\"\\\"\\\"Process URLs in batches and extract features.\\\"\\\"\\\"\\n\",\n",
    "    \"    if urls is None:\\n\",\n",
    "    \"        return\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Remove duplicates and filter\\n\",\n",
    "    \"    urls_to_process = urls[~urls[\\\"url\\\"].isin(handled_urls[\\\"url\\\"])]\\n\",\n",
    "    \"    urls_to_process = urls_to_process[urls_to_process[\\\"is_spam\\\"] == False]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Processing {len(urls_to_process)} new URLs\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    tasks = []\\n\",\n",
    "    \"    batch_size = 1000\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i in tqdm(range(0, len(urls_to_process), batch_size)):\\n\",\n",
    "    \"        batch = urls_to_process.iloc[i : i + batch_size]\\n\",\n",
    "    \"        for _, record in batch.iterrows():\\n\",\n",
    "    \"            url = record[\\\"url\\\"]\\n\",\n",
    "    \"            tasks.append(get_features(url))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        results = await gather(*tasks, limit=500)\\n\",\n",
    "    \"        dataset = pd.DataFrame(results)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Save progress\\n\",\n",
    "    \"        dataset.to_csv(\\\"data/dataset.csv\\\", mode=\\\"a\\\", header=False, index=False)\\n\",\n",
    "    \"        tasks = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"Processed {min(i + batch_size, len(urls_to_process))} URLs\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Run feature extraction\\n\",\n",
    "    \"await validate_urls()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. BERT URL Classification\\n\",\n",
    "    \"\\n\",\n",
    "    \"Initialize and use the BERT model for URL classification.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def initialize_bert_model():\\n\",\n",
    "    \"    \\\"\\\"\\\"Initialize BERT model and tokenizer.\\\"\\\"\\\"\\n\",\n",
    "    \"    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"    model_name = \\\"CrabInHoney/urlbert-tiny-v4-phishing-classifier\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Using device: {device}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    tokenizer = BertTokenizerFast.from_pretrained(model_name)\\n\",\n",
    "    \"    model = BertForSequenceClassification.from_pretrained(model_name)\\n\",\n",
    "    \"    model.to(device)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    classifier = pipeline(\\n\",\n",
    "    \"        \\\"text-classification\\\",\\n\",\n",
    "    \"        model=model,\\n\",\n",
    "    \"        tokenizer=tokenizer,\\n\",\n",
    "    \"        device=0 if torch.cuda.is_available() else -1,\\n\",\n",
    "    \"        return_all_scores=True\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return classifier\\n\",\n",
    "    \"\\n\",\n",
    "    \"async def add_url_bert_classification():\\n\",\n",
    "    \"    \\\"\\\"\\\"Add BERT classification scores to the dataset.\\\"\\\"\\\"\\n\",\n",
    "    \"    classifier = initialize_bert_model()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    urls = pd.read_csv(\\\"data/urls_with_features.csv\\\")\\n\",\n",
    "    \"    urls[\\\"url_bert_classification\\\"] = None\\n\",\n",
    "    \"    unhandled_urls = urls[urls[\\\"url_bert_classification\\\"].isna()]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Processing {len(unhandled_urls)} URLs with BERT\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for index, record in tqdm(unhandled_urls.iterrows(), total=len(unhandled_urls)):\\n\",\n",
    "    \"        url = record[\\\"url\\\"]\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            result = classifier(url)\\n\",\n",
    "    \"            label_1_score = result[0][1]['score']\\n\",\n",
    "    \"            urls.at[index, \\\"url_bert_classification\\\"] = label_1_score\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            urls.at[index, \\\"url_bert_classification\\\"] = str(e)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if index % 100 == 0:\\n\",\n",
    "    \"            urls.to_csv(\\\"data/urls_with_features_and_url_classification.csv\\\", index=False)\\n\",\n",
    "    \"            print(f\\\"Saved progress at {index} URLs\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Run BERT classification\\n\",\n",
    "    \"await add_url_bert_classification()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Model Training and Evaluation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Prepare the data and train the XGBoost model.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Placeholder for model training and evaluation code\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.10\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "f8b6bb4ef334235"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6fa2fdf08211aa0d"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
